[[36m2022-04-26 11:31:24,751[0m][[34msrc.utils[0m][[32mINFO[0m] - Disabling python warnings! <config.ignore_warnings=True>[0m
[[36m2022-04-26 11:31:24,751[0m][[34msrc.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <config.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.fno_2d_datamodule.FNO2dDataModule             
â”‚       dataset:                                                                
â”‚         _target_: src.datamodules.fno_2d_datamodule.FNO2dDataset              
â”‚         data_path: /disk/student/dvoytan/FNO_acoustic_wave_propogation/data/la
â”‚         n_models: 4000                                                        
â”‚         x_shape:                                                              
â”‚         - 0                                                                   
â”‚         - 128.01                                                              
â”‚         - 0.01                                                                
â”‚         y_shape:                                                              
â”‚         - 0                                                                   
â”‚         - 128.01                                                              
â”‚         - 0.01                                                                
â”‚         reshape_shape:                                                        
â”‚         - 128                                                                 
â”‚         - 128                                                                 
â”‚         stage: train                                                          
â”‚         y_rescale: 10000.0                                                    
â”‚         train_freqs:                                                          
â”‚         - 0.5                                                                 
â”‚         - 10.1                                                                
â”‚         - 0.5                                                                 
â”‚         test_freqs:                                                           
â”‚         - 0.25                                                                
â”‚         - 10.25                                                               
â”‚         - 0.5                                                                 
â”‚       batch_size: 32                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 3500                                                                  
â”‚       - 250                                                                   
â”‚       - 250                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.fno_2D_module.FNO2dModule                          
â”‚       lr: 0.001                                                               
â”‚       weight_decay: 0.0001                                                    
â”‚       loss:                                                                   
â”‚         _target_: src.utils.fno_utilities.LpLoss                              
â”‚         size_average: false                                                   
â”‚       net:                                                                    
â”‚         _target_: src.models.components.fno_2d.FNO2d                          
â”‚         modes1: 12                                                            
â”‚         modes2: 12                                                            
â”‚         width: 32                                                             
â”‚         padding: 9                                                            
â”‚         in_channels: 3                                                        
â”‚         outsize:                                                              
â”‚         - 128                                                                 
â”‚         - 2                                                                   
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         monitor: val/loss                                                     
â”‚         mode: min                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: true                                                       
â”‚         verbose: false                                                        
â”‚         dirpath: checkpoints/                                                 
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         auto_insert_metric_name: false                                        
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/loss                                                     
â”‚         mode: max                                                             
â”‚         patience: 20                                                          
â”‚         min_delta: 0                                                          
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: pytorch_lightning.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: tensorboard/                                                
â”‚         name: null                                                            
â”‚         version: default                                                      
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       strategy: ddp                                                           
â”‚       gpus:                                                                   
â”‚       - 0                                                                     
â”‚       - 1                                                                     
â”‚       - 2                                                                     
â”‚       - 3                                                                     
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 100                                                         
â”‚       num_sanity_val_steps: 1                                                 
â”‚       resume_from_checkpoint: null                                            
â”‚                                                                               
â”œâ”€â”€ original_work_dir
â”‚   â””â”€â”€ /disk/student/dvoytan/FNO_acoustic_wave_propogation                     
â”œâ”€â”€ data_dir
â”‚   â””â”€â”€ /disk/student/dvoytan/FNO_acoustic_wave_propogation/data/               
â”œâ”€â”€ print_config
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ignore_warnings
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ name
    â””â”€â”€ default                                                                 
[[36m2022-04-26 11:31:24,910[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.fno_2d_datamodule.FNO2dDataModule>[0m
[[36m2022-04-26 11:31:25,235[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating model <src.models.fno_2D_module.FNO2dModule>[0m
[[36m2022-04-26 11:31:26,169[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpy1qrk2s1[0m
[[36m2022-04-26 11:31:26,170[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpy1qrk2s1/_remote_module_non_sriptable.py[0m
[[36m2022-04-26 11:31:26,217[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-04-26 11:31:26,219[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2022-04-26 11:31:26,220[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-04-26 11:31:26,220[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-04-26 11:31:26,221[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2022-04-26 11:31:26,222[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-04-26 11:31:26,244[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-04-26 11:31:26,245[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-04-26 11:31:26,245[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-04-26 11:31:26,245[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-04-26 11:31:26,245[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-04-26 11:31:26,245[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-04-26 11:31:26,255[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting training![0m
[[36m2022-04-26 11:31:29,660[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpkbthkkxl[0m
[[36m2022-04-26 11:31:29,661[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpkbthkkxl/_remote_module_non_sriptable.py[0m
[[36m2022-04-26 11:31:29,826[0m][[34mpytorch_lightning.utilities.distributed[0m][[32mINFO[0m] - Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4[0m
[[36m2022-04-26 11:31:33,320[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpejwykccj[0m
[[36m2022-04-26 11:31:33,321[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpejwykccj/_remote_module_non_sriptable.py[0m
[[36m2022-04-26 11:31:33,463[0m][[34mpytorch_lightning.utilities.distributed[0m][[32mINFO[0m] - Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4[0m
[[36m2022-04-26 11:31:36,044[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpg7bzga2q[0m
[[36m2022-04-26 11:31:36,044[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpg7bzga2q/_remote_module_non_sriptable.py[0m
[[36m2022-04-26 11:31:36,140[0m][[34mpytorch_lightning.utilities.distributed[0m][[32mINFO[0m] - Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4[0m
[[36m2022-04-26 11:31:37,904[0m][[34mpytorch_lightning.utilities.distributed[0m][[32mINFO[0m] - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4[0m
[[36m2022-04-26 11:31:38,142[0m][[34mtorch.distributed.distributed_c10d[0m][[32mINFO[0m] - Added key: store_based_barrier_key:1 to store for rank: 3[0m
[[36m2022-04-26 11:31:38,466[0m][[34mtorch.distributed.distributed_c10d[0m][[32mINFO[0m] - Added key: store_based_barrier_key:1 to store for rank: 2[0m
[[36m2022-04-26 11:31:38,830[0m][[34mtorch.distributed.distributed_c10d[0m][[32mINFO[0m] - Added key: store_based_barrier_key:1 to store for rank: 1[0m
[[36m2022-04-26 11:31:38,840[0m][[34mtorch.distributed.distributed_c10d[0m][[32mINFO[0m] - Added key: store_based_barrier_key:1 to store for rank: 0[0m
[[36m2022-04-26 11:31:38,841[0m][[34mtorch.distributed.distributed_c10d[0m][[32mINFO[0m] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.[0m
[[36m2022-04-26 11:31:38,841[0m][[34mtorch.distributed.distributed_c10d[0m][[32mINFO[0m] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.[0m
[[36m2022-04-26 11:31:38,841[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------
[0m
[[36m2022-04-26 11:31:38,846[0m][[34mtorch.distributed.distributed_c10d[0m][[32mINFO[0m] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.[0m
[[36m2022-04-26 11:31:38,850[0m][[34mtorch.distributed.distributed_c10d[0m][[32mINFO[0m] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.[0m
[[36m2022-04-26 11:31:43,800[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3][0m
[[36m2022-04-26 11:31:43,800[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3][0m
[[36m2022-04-26 11:31:43,800[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3][0m
[[36m2022-04-26 11:31:43,800[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name      â”ƒ Type           â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net       â”‚ FNO2d          â”‚  2.4 M â”‚
â”‚ 1  â”‚ net.fc0   â”‚ Linear         â”‚    128 â”‚
â”‚ 2  â”‚ net.conv0 â”‚ SpectralConv2d â”‚  589 K â”‚
â”‚ 3  â”‚ net.conv1 â”‚ SpectralConv2d â”‚  589 K â”‚
â”‚ 4  â”‚ net.conv2 â”‚ SpectralConv2d â”‚  589 K â”‚
â”‚ 5  â”‚ net.conv3 â”‚ SpectralConv2d â”‚  589 K â”‚
â”‚ 6  â”‚ net.w0    â”‚ Conv2d         â”‚  1.1 K â”‚
â”‚ 7  â”‚ net.w1    â”‚ Conv2d         â”‚  1.1 K â”‚
â”‚ 8  â”‚ net.w2    â”‚ Conv2d         â”‚  1.1 K â”‚
â”‚ 9  â”‚ net.w3    â”‚ Conv2d         â”‚  1.1 K â”‚
â”‚ 10 â”‚ net.fc1   â”‚ Linear         â”‚  4.2 K â”‚
â”‚ 11 â”‚ net.fc2   â”‚ Linear         â”‚    258 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 2.4 M                                                         
Non-trainable params: 0                                                         
Total params: 2.4 M                                                             
Total estimated model params size (MB): 9                                       
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch 20  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 587/587 0:07:05 â€¢ 0:00:00 2.23it/s loss: 3.9 v_num: 
                                                               ault             
[[36m2022-04-26 13:54:54,105[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Starting testing![0m
[[36m2022-04-26 13:54:54,128[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Restoring states from the checkpoint path at /disk/student/dvoytan/FNO_acoustic_wave_propogation/logs/experiments/runs/default/2022-04-26_11-31-23/checkpoints/epoch_020.ckpt[0m
[[36m2022-04-26 13:54:54,162[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3][0m
[[36m2022-04-26 13:54:54,163[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3][0m
[[36m2022-04-26 13:54:54,166[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3][0m
[[36m2022-04-26 13:54:54,166[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3][0m
[[36m2022-04-26 13:54:54,182[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Loaded model weights from checkpoint at /disk/student/dvoytan/FNO_acoustic_wave_propogation/logs/experiments/runs/default/2022-04-26_11-31-23/checkpoints/epoch_020.ckpt[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚     4.187572479248047     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40/40 0:00:41 â€¢ 0:00:00 0.94it/s 
[[36m2022-04-26 13:55:37,045[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Finalizing![0m
[[36m2022-04-26 13:55:37,046[0m][[34msrc.training_pipeline[0m][[32mINFO[0m] - Best model ckpt at /disk/student/dvoytan/FNO_acoustic_wave_propogation/logs/experiments/runs/default/2022-04-26_11-31-23/checkpoints/epoch_020.ckpt[0m
